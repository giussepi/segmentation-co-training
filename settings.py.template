# -*- coding: utf-8 -*-
""" settings """

import os
import logging

import torch
from gtorch_utils.segmentation import torchmetrics
from monai import transforms as ts

from nns.utils.metrics import MetricItem

###############################################################################
#                                GENERAL CONFIG                               #
###############################################################################

BASE_PATH = os.getenv("HOME")
PROJECT_PATH = os.path.join(BASE_PATH, '<path to your project>')
# https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html#all-together-automatic-mixed-precision
USE_AMP = False  # Automatic Mixed Precision
LOG_LEVEL = logging.INFO
CUDA = True
MULTIGPUS = True
PATCH_REPLICATION_CALLBACK = True
COTRAINING = True
# ORIGINAL_MASKS: if True the CoTraining will merge the original masks
#                 (not the co-training masks) with the new predictions;
#                 otherwise, the co-training masks will be used (normal workflow)
ORIGINAL_MASKS = True
# Metrics to be used by ModelMGR and CoTraining
METRICS = [
    MetricItem(torchmetrics.DiceCoefficient(), main=True),
    MetricItem(torchmetrics.Specificity()),
    # MetricItem(torchmetrics.Recall()),
    MetricItem(torchmetrics.Accuracy()),
    MetricItem(torchmetrics.BalancedAccuracy()),
]
DISABLE_PROGRESS_BAR = False

if CUDA == MULTIGPUS == True and torch.cuda.is_available():
    NUM_GPUS = torch.cuda.device_count()
else:
    NUM_GPUS = 1  # When working with only 1 GPU or CPU

BATCH_SIZE = 16
TOTAL_BATCH_SIZE = BATCH_SIZE * NUM_GPUS
NUM_WORKERS = 16  # set it to 0 when debugging the dataset
DB_CACHE = False  # The dataset should receive it as argument

###############################################################################
#                            utils/sync_batchnorm/                             #
###############################################################################

DATA_DIMENSIONS = 3

###############################################################################
#                                     NNS                                     #
###############################################################################

DIR_CHECKPOINTS = os.path.join(PROJECT_PATH, 'checkpoints')
PLOT_DIRECTORY = os.path.join(PROJECT_PATH, 'plots')

###############################################################################
#                                    CONSEP                                    #
###############################################################################

PATCH_SIZE = (540, 540)
PATCH_STEP_SIZE = (164, 164)
# crops are extracted from the patches
CROP_IMG_SHAPE = (256, 256)
CROP_MASK_SHAPE = (256, 256)  # (80, 80)
CREATEDATASET_SAVING_PATH = 'consep_dataset'
# CoNSeP (dataset created after running consep.processors.offline.CreateDataset)
CONSEP_TRAIN_PATH = os.path.join(PROJECT_PATH, CREATEDATASET_SAVING_PATH, 'train')
CONSEP_VAL_PATH = os.path.join(PROJECT_PATH, CREATEDATASET_SAVING_PATH, 'val')
CONSEP_TEST_PATH = ''

###############################################################################
#                                     ct82                                    #
###############################################################################

CT82_SIZE = [368, 368, -1]  # [height, width, scans]
CT82_CROP_SHAPE = [32, 80, 80]  # [46, 80, 80] [96, 160, 160] [scans, heigh, width]
CT82_NUM_CROPS = 3

CT82_TRANSFORMS = {
    'train': ts.Compose([
        ts.ToTensord(keys=['img', 'mask']),
        # ts.AsChannelFirstd(keys=['img', 'mask'], channel_dim=-1),
        # ts.AddChanneld(keys=['img', 'mask']),
        ts.CropForegroundd(keys=['img', 'mask'], source_key='img', select_fn=lambda x: x > 0),
        ts.RandAxisFlipd(keys=['img', 'mask'], prob=.5),
        ts.RandAffined(
            keys=['img', 'mask'],
            prob=1.,
            rotate_range=0.261799,  # 15 degrees
            translate_range=[0*CT82_SIZE[2], 0.1*CT82_SIZE[0], 0.1*CT82_SIZE[1]],
            scale_range=((-0.3,  0.3), (-0.3, 0.3), (-0.3, 0.3)),
            mode=["bilinear", "nearest"]
        ),
        ts.RandCropByPosNegLabeld(
            keys=['img', 'mask'],
            label_key='mask',
            spatial_size=CT82_CROP_SHAPE,
            pos=.5,
            neg=.5,
            num_samples=CT82_NUM_CROPS,
        ),
        # ts.RandSpatialCropd(
        #     keys=['img', 'mask'], roi_size=CT82_CROP_SHAPE, random_center=True, random_size=False)
        # ts.AsChannelLastd(keys=['img', 'mask'], channel_dim=1),
        # ts.SqueezeDimd(keys=['img', 'mask'])
    ]),
    'valtest': ts.Compose([
        ts.ToTensord(keys=['img', 'mask']),
        # ts.AsChannelFirstd(keys=['img', 'mask'], channel_dim=-1),
        # ts.AddChanneld(keys=['img', 'mask']),
        ts.CropForegroundd(keys=['img', 'mask'], source_key='img', select_fn=lambda x: x > 0),
        ts.RandCropByPosNegLabeld(
            keys=['img', 'mask'],
            label_key='mask',
            spatial_size=CT82_CROP_SHAPE,
            pos=.5,
            neg=.5,
            num_samples=CT82_NUM_CROPS,
        ),
        # ts.RandSpatialCropd(
        #     keys=['img', 'mask'], roi_size=CT82_CROP_SHAPE, random_center=False, random_size=False)
        # ts.AsChannelLastd(keys=['img', 'mask'], channel_dim=1),
        # ts.SqueezeDimd(keys=['img', 'mask'])
    ])
}

CT82_TRAIN_PATH = os.path.join(PROJECT_PATH, 'CT-82-Pro', 'train')
CT82_VAL_PATH = os.path.join(PROJECT_PATH, 'CT-82-Pro', 'val')
CT82_TEST_PATH = os.path.join(PROJECT_PATH, 'CT-82-Pro', 'test')

###############################################################################
#                                    LiTS17                                   #
###############################################################################

LITS17_SIZE = CT82_SIZE
LITS17_CROP_SHAPE = CT82_CROP_SHAPE
LITS17_NUM_CROPS = CT82_NUM_CROPS
LITS17_TRANSFORMS = CT82_TRANSFORMS
LITS17_TRAIN_PATH = os.path.join(PROJECT_PATH, 'LiTS17-Pro', 'train')
LITS17_VAL_PATH = os.path.join(PROJECT_PATH, 'LiTS17-Pro', 'val')
LITS17_TEST_PATH = os.path.join(PROJECT_PATH, 'LiTS17-Pro', 'test')

###############################################################################
#                                  BACKBONES                                  #
###############################################################################

BACKBONES_BN_MOM = .1
BACKBONES_MEAN = (.485, .456, .406)
BACKBONES_STD = (.229, .224, .225)
BACKBONES_DIR_CHECKPOINTS = os.path.join(DIR_CHECKPOINTS, 'backbones')

# NOTE: If using xception, then manually download it from
# https://drive.google.com/file/d/1_j_mE07tiV24xXOJw4XDze0-a0NAhNVi/view
# and place it in the right directory

BACKBONES_MODEL_URLS = {
    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',
    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',
    'resnet50': 'https://s3.us-west-1.wasabisys.com/encoding/models/resnet50s-a75c83cf.zip',
    'resnet101': 'https://s3.us-west-1.wasabisys.com/encoding/models/resnet101s-03a0f310.zip',
    'resnet152': 'https://s3.us-west-1.wasabisys.com/encoding/models/resnet152s-36670e8b.zip',
    'xception': os.path.join(BACKBONES_DIR_CHECKPOINTS, 'xception_pytorch_imagenet.pth'),
}

###############################################################################
#                                  DEBUGGING                                  #
###############################################################################

DEBUG = False

if DEBUG:
    # Add any extra debugging behaviour here
    LOG_LEVEL = logging.DEBUG
    # MULTIGPUS = False
    # NUM_GPUS = 1
    # TOTAL_BATCH_SIZE = BATCH_SIZE * NUM_GPUS
    NUM_WORKERS = 0  # necessary to properly use the breakpoints
