# -*- coding: utf-8 -*-
""" consep/dataloaders/train_loader """

import os
import random
import re

import numpy as np
import torch
import torch.utils.data
from gtorch_utils.constants import DB
from gtorch_utils.datasets.segmentation import DatasetTemplate
from imgaug import augmenters as iaa
from PIL import Image

from consep.utils.utils import cropping_center
from consep.utils.augmentations import (
    add_to_brightness,
    add_to_contrast,
    add_to_hue,
    add_to_saturation,
    gaussian_blur,
    median_blur,
)


class FileLoader(torch.utils.data.Dataset):
    """
    Data Loader. Loads images from a file list and
    performs augmentation with the albumentation library.

    Args:
        file_list       <list>: list of .npy filenames to load (generated by
                                consep.utils.patches.patches import ProcessDataset)
        input_shape    <tuple>: shape of the network input [h,w] - defined in config.py
        mask_shape     <tuple>: shape of the network output [h,w] - defined in config.py
        mode             <str>: subdataset (see gtorch_utils.constants.py -> DB class). Default DB.TRAIN
        setup_augmentor <bool>: Whether or not perform data augmentation. Default=True

    Inspired on: https://github.com/vqdang/hover_net/blob/master/dataloader/train_loader.py

    Usage:
        train_path = 'dataset/training_data/consep/train/540x540_164x164'
        train_list = glob.glob(os.path.join(train_path, '*.npy'))
        train_list.sort()

        input_dataset = FileLoader(
            file_list=train_list,
            input_shape=model_input_shape,
            mask_shape=model_outut_shape,
            mode=DB.TRAIN,
            setup_augmentor=True,
        )

        train_dataloader = DataLoader(
            input_dataset,
            num_workers=num_workers,
            batch_size=batch_size * num_gpus,
            shuffle=run_mode == DB.TRAIN,
            drop_last=run_mode == DB.TRAIN,
            **SeedWorker(preserve_reproductibility=True)(),
        )
    """

    def __init__(self, **kwargs):
        """ Initializes the object instance """

        self.info_list = kwargs.get('file_list')
        self.input_shape = kwargs.get('input_shape')
        self.mask_shape = kwargs.get('mask_shape')
        self.mode = kwargs.get('mode', DB.TRAIN)
        setup_augmentor = kwargs.get('setup_augmentor', True)

        assert isinstance(self.info_list, list), type(self.info_list)
        assert len(self.info_list) > 0, 'info_list cannot be empty'
        assert isinstance(self.input_shape, tuple), type(self.input_shape)
        assert isinstance(self.mask_shape, tuple), type(self.mask_shape)
        DB.clean_subdataset_name(self.mode)

        self.id = 0
        self.shape_augs = self.input_augs = None

        if setup_augmentor:
            self.setup_augmentor(0, 0)

    def __len__(self):
        return len(self.info_list)

    def __get_augmentation(self, mode, rng):
        if mode == DB.TRAIN:
            shape_augs = [
                # * order = ``0`` -> ``cv2.INTER_NEAREST``
                # * order = ``1`` -> ``cv2.INTER_LINEAR``
                # * order = ``2`` -> ``cv2.INTER_CUBIC``
                # * order = ``3`` -> ``cv2.INTER_CUBIC``
                # * order = ``4`` -> ``cv2.INTER_CUBIC``
                # ! for pannuke v0, no rotation or translation, just flip to avoid mirror padding
                iaa.Affine(
                    # scale images to 80-120% of their size, individually per axis
                    scale={"x": (0.8, 1.2), "y": (0.8, 1.2)},
                    # translate by -A to +A percent (per axis)
                    translate_percent={"x": (-0.01, 0.01), "y": (-0.01, 0.01)},
                    shear=(-5, 5),  # shear by -5 to +5 degrees
                    rotate=(-179, 179),  # rotate by -179 to +179 degrees
                    order=0,  # use nearest neighbour
                    backend="cv2",  # opencv for fast processing
                    seed=rng,
                ),
                # set position to 'center' for center crop
                # else 'uniform' for random crop
                iaa.CropToFixedSize(
                    self.input_shape[0], self.input_shape[1], position="center"
                ),
                iaa.Fliplr(0.5, seed=rng),
                iaa.Flipud(0.5, seed=rng),
            ]

            input_augs = [
                iaa.OneOf(
                    [
                        iaa.Lambda(
                            seed=rng,
                            func_images=lambda *args: gaussian_blur(*args, max_ksize=3),
                        ),
                        iaa.Lambda(
                            seed=rng,
                            func_images=lambda *args: median_blur(*args, max_ksize=3),
                        ),
                        iaa.AdditiveGaussianNoise(
                            loc=0, scale=(0.0, 0.05 * 255), per_channel=0.5
                        ),
                    ]
                ),
                iaa.Sequential(
                    [
                        iaa.Lambda(
                            seed=rng,
                            func_images=lambda *args: add_to_hue(*args, range=(-8, 8)),
                        ),
                        iaa.Lambda(
                            seed=rng,
                            func_images=lambda *args: add_to_saturation(
                                *args, range=(-0.2, 0.2)
                            ),
                        ),
                        iaa.Lambda(
                            seed=rng,
                            func_images=lambda *args: add_to_brightness(
                                *args, range=(-26, 26)
                            ),
                        ),
                        iaa.Lambda(
                            seed=rng,
                            func_images=lambda *args: add_to_contrast(
                                *args, range=(0.75, 1.25)
                            ),
                        ),
                    ],
                    random_order=True,
                ),
            ]
        elif mode in (DB.VALIDATION, DB.TEST):
            shape_augs = [
                # set position to 'center' for center crop
                # else 'uniform' for random crop
                iaa.CropToFixedSize(
                    self.input_shape[0], self.input_shape[1], position="center"
                )
            ]
            input_augs = []

        return shape_augs, input_augs

    def setup_augmentor(self, worker_id, seed):
        """ Configurates the data augmentation procedure """
        self.augmentor = self.__get_augmentation(self.mode, seed)
        self.shape_augs = iaa.Sequential(self.augmentor[0])
        self.input_augs = iaa.Sequential(self.augmentor[1])
        self.id = self.id + worker_id

    def __getitem__(self, idx):
        """
        Loads the image and annotations/mask from the object with id=idx, performs data augmentation,
        extracts crops from the centre, and returns a dictionary with the image and its binary mask

        Returns:
            {'img': img, 'mask': binary_mask}
        """
        path = self.info_list[idx]
        data = np.load(path)

        # split stacked channel into image and label
        img = (data[..., :3]).astype("uint8")  # RGB images
        ann = (data[..., 3:]).astype("int32")  # instance ID map and type map

        if self.shape_augs is not None:
            shape_augs = self.shape_augs.to_deterministic()
            img = shape_augs.augment_image(img)
            ann = shape_augs.augment_image(ann)

        if self.input_augs is not None:
            input_augs = self.input_augs.to_deterministic()
            img = input_augs.augment_image(img)

        img = cropping_center(img, self.input_shape)
        feed_dict = {"img": img}
        inst_map = (ann[..., 0]).copy()  # HW1 -> HW
        inst_map[inst_map > 0] = 1
        feed_dict["mask"] = cropping_center(inst_map, self.mask_shape)

        return feed_dict


class CoNSePDataset(DatasetTemplate):
    """
    Dataset for the image CoNSeP dataset created through
    consep.processors.offline->CreateDataset

    Usage:

    train, val, test = CoNSePDataset.get_subdatasets(
        train_path='consep_dataset/train', val_path='consep_dataset/val')

    train_dataloader = DataLoader(
        train,
        num_workers=0,
        batch_size=batch_size * num_gpus,
        shuffle=run_mode == DB.TRAIN,
        drop_last=run_mode == DB.TRAIN,
        **SeedWorker(preserve_reproductibility=True)(),
    )
    """

    NUM_CLASSES = 2

    def __init__(self, **kwargs):
        """
        Initializes the object instance

        Kwargs:
            images_masks_path <str>: path to the folder for containing images and masks
            filename_reg      <str>: regular expression to get the index id from the crop filename.
                                     Default r'(?P<filename>\d+).ann.tiff'
            image_extension   <str>: image extension. Default '.ann.tiff'
            mask_extension    <str>: mask extension. Default '.mask.png'
        """
        self.images_masks_path = kwargs.get('images_masks_path')
        self.filename_reg = kwargs.get('filename_reg', r'(?P<filename>\d+).ann.tiff')
        self.image_extension = kwargs.get('image_extension', '.ann.tiff')
        self.mask_extension = kwargs.get('mask_extension', '.mask.png')

        assert isinstance(self.images_masks_path, str), type(self.images_masks_path)
        assert os.path.isdir(self.images_masks_path), self.images_masks_path
        assert isinstance(self.filename_reg, str), type(self.filename_reg)
        assert isinstance(self.image_extension, str), type(self.image_extension)
        assert isinstance(self.mask_extension, str), type(self.mask_extension)

        self.pattern = re.compile(self.filename_reg)
        self.image_list = [file_ for file_ in os.listdir(self.images_masks_path) if bool(self.pattern.fullmatch(file_))]

    def __len__(self):
        return len(self.image_list)

    def get_image_and_mask_files(self, idx):
        """
        Loads the image and mask corresponding to the file at position idx in the image list.
        Besides, both are properly formatted to be used by the neuronal network before
        returning them.

        Args:
            idx <int>: image index
        Returns:
            image <np.ndarray>, target_mask <np.ndarray>
        """
        assert isinstance(idx, int), type(idx)

        image = Image.open(os.path.join(self.images_masks_path, self.image_list[idx]))
        mask = Image.open(os.path.join(
            self.images_masks_path,
            self.image_list[idx].replace(self.image_extension, self.mask_extension)
        ))

        assert image.size == mask.size, \
            f'Image and mask {idx} should be the same size, but are {image.size} and {mask.size}'

        image = np.array(image.convert('RGB')) if image.mode != 'RGB' else np.array(image)
        mask = np.array(mask.convert('L')) if mask.mode != 'L' else np.array(mask)
        target_mask = np.zeros((*mask.shape[:2], self.NUM_CLASSES), dtype=np.float32)
        target_mask[..., 1] = (mask == 255).astype(np.float32)  # nuclei class
        target_mask[..., 0] = 1 - target_mask[..., 1]  # other class

        return image, target_mask, '', '', ''

    @staticmethod
    def preprocess(img):
        """
        Preprocess the image and returns it

        Args:
            img <np.ndarray>:
        Returns:
            image <np.ndarray>
        """
        assert isinstance(img, np.ndarray), type(img)

        # HWC to CHW
        img = img.transpose(2, 0, 1)

        if img.max() > 1:
            img = img / 255

        return img

    @classmethod
    def get_subdatasets(cls, **kwargs):
        """
        Creates and returns train, validation and test subdatasets to be used with DataLoaders

        Kwargs:
            train_path      <str>: path to the folder for containing training images and masks
            val_path        <str>: path to the folder for containing validation images and masks
            test_path       <str>: path to the folder for containing testing images and masks. Default ''
            filename_reg    <str>: regular expression to get the index id from the crop filename.
                                   Default r'(?P<filename>\d+).ann.tiff'
            image_extension <str>: image extension. Default '.ann.tiff'
            mask_extension  <str>: mask extension. Default '.mask.png'

        Returns:
           train <CoNSePDataset>, validation <CoNSePDataset>, test <CoNSePDataset or None>
        """
        train_path = kwargs.pop('train_path')
        val_path = kwargs.pop('val_path')
        test_path = kwargs.pop('test_path', '')

        # making sure this keyword argument is not passed when creating the instances
        if 'images_masks_path' in kwargs:
            kwargs.pop('images_masks_path')

        train_dataset = cls(images_masks_path=train_path, **kwargs)
        val_dataset = cls(images_masks_path=val_path, **kwargs)
        test_dataset = cls(images_masks_path=test_path, **kwargs) if test_path else None

        return train_dataset, val_dataset, test_dataset


class SeedWorker:
    """
    Returns arguments for the DataLoader to produce deterministic or stochastic results

    Usage:
        DataLoader(
            dataset,
            num_workers=num_workers,
            batch_size=batch_size,
            **SeedWorker(preserve_reproductibility=True)()
        )
    """

    def __init__(self, preserve_reproductibility=False, seed=10):
        """
        Initializes the object instance

        Args:
            preserve_reproductibility <bool>: Whether or not have reproductible results. Default False
            seed <int>: Seed for the generator created when preserve_reproductibility = True
        """
        assert isinstance(preserve_reproductibility, bool), type(preserve_reproductibility)
        assert isinstance(seed, int), type(seed)
        assert seed >= 0

        self.preserve_reproductibility = preserve_reproductibility
        self.seed = seed

    def __call__(self):
        """ Functor call """
        if self.preserve_reproductibility:
            generator = torch.Generator()
            generator.manual_seed(self.seed)

            return {
                'worker_init_fn': self.deterministic,
                'generator': generator
            }

        return {'worker_init_fn': self.nondeterministic}

    @staticmethod
    def deterministic(worked_id):
        """
        Initializes the seed to preserve reproductibility

        Source: https://pytorch.org/docs/master/notes/randomness.html#dataloader
        """
        worker_seed = torch.initial_seed() % 2**32
        np.random.seed(worker_seed)
        random.seed(worker_seed)

    @staticmethod
    def nondeterministic(worker_id):
        """
        Makes sure the initialization are truly random

        Initializes the augmentor per worker, else duplicated rng generators may happen

        Source: https://github.com/vqdang/hover_net/blob/master/run_train.py#L48
        """
        # ! to make the seed chain reproducible, must use the torch random, not numpy
        # the torch rng from main thread will regenerate a base seed, which is then
        # copied into the dataloader each time it created (i.e start of each epoch)
        # then dataloader with this seed will spawn worker, now we reseed the worker
        worker_info = torch.utils.data.get_worker_info()
        # to make it more random, simply switch torch.randint to np.randint
        worker_seed = torch.randint(0, 2 ** 32, (1,))[0].cpu().item() + worker_id
        # print('Loader Worker %d Uses RNG Seed: %d' % (worker_id, worker_seed))
        # retrieve the dataset copied into this worker process
        # then set the random seed for each augmentation
        #
        worker_info.dataset.setup_augmentor(worker_id, worker_seed)
